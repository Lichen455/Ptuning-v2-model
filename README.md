## Using a model fine-tuned with partial programming problems for chatglm2 ##

Used some small problems from Luogu and Codeforces, controlled the number of words to be between 100-200.

## 该仓库用来存放进行微调的各种表现好的模型 ##




### 使chatglm2用部分编程题目ptuning微调的一个模型 ###   

output11/adgen-chatglm2-6b-pt-128-2e-2 使用洛谷codeforce算法题目（100-200字）数据集P-tuning v2微调的glm2模型,在各方面成绩较好

  
使用了洛谷codeforce的一些小题目，控制了字数100-200
